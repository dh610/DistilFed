{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dh610/DistilFed/blob/main/Flower_with_berttiny.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hM3IuO9z6HZ"
      },
      "outputs": [],
      "source": [
        "#----- 필요 라이브러리입니다.-----\n",
        "#     \"flwr[simulation]==1.12.0\",\n",
        "#     \"flwr-datasets>=0.3.0\",\n",
        "#     \"torch==2.4.0\",\n",
        "#     \"transformers>=4.30.0,<5.0\",\n",
        "#     \"evaluate>=0.4.0,<1.0\",\n",
        "#     \"datasets>=2.0.0, <3.0\",\n",
        "#     \"scikit-learn>=1.3.1, <2.0\","
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 코드는 \"https://github.com/adap/flower/tree/main/examples/quickstart-huggingface\"를 참고했습니다."
      ],
      "metadata": {
        "id": "secnu3WcCLYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q \"flwr[simulation]\" flwr-datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmnqIj0074pa",
        "outputId": "f6f292f1-d083-4d92-b438-4de6f151807e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.1/65.1 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m99.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m100.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m512.2/512.2 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch transformers evaluate datasets scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pq7LfRWo8SEn",
        "outputId": "23515fe7-90c3-4839-cf74-9f7018afcc96"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/84.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/480.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m471.0/480.6 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any\n",
        "from collections import OrderedDict\n",
        "\n",
        "import torch\n",
        "from evaluate import load as load_metric\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    DataCollatorWithPadding,\n",
        "    AutoModelForSequenceClassification,\n",
        ")\n",
        "from datasets.utils.logging import disable_progress_bar\n",
        "from flwr_datasets import FederatedDataset\n",
        "from flwr_datasets.partitioner import IidPartitioner\n"
      ],
      "metadata": {
        "id": "Rinr-S4D8PkP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "disable_progress_bar()\n",
        "fds = None  # Cache FederatedDataset\n",
        "\n",
        "def load_data(\n",
        "    partition_id: int, num_partitions: int, model_name: str\n",
        ") -> tuple[DataLoader[Any], DataLoader[Any]]:\n",
        "    \"\"\"Load IMDB data (training and eval)\"\"\"\n",
        "    # Only initialize `FederatedDataset` once\n",
        "\n",
        "    # fds는 분리한 데이터셋으로 client가 접근 가능한 데이터셋이라고 생각하면 됩니다.\n",
        "    global fds\n",
        "    if fds is None:\n",
        "        # Partition the IMDB dataset into N partitions\n",
        "        partitioner = IidPartitioner(num_partitions=num_partitions)\n",
        "        fds = FederatedDataset(\n",
        "            dataset=\"stanfordnlp/imdb\", partitioners={\"train\": partitioner}\n",
        "        )\n",
        "    partition = fds.load_partition(partition_id)\n",
        "    # Divide data: 80% train, 20% test\n",
        "    partition_train_test = partition.train_test_split(test_size=0.2, seed=42)\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=512)\n",
        "\n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(examples[\"text\"], truncation=True, add_special_tokens=True)\n",
        "\n",
        "    partition_train_test = partition_train_test.map(tokenize_function, batched=True)\n",
        "    partition_train_test = partition_train_test.remove_columns(\"text\")\n",
        "    partition_train_test = partition_train_test.rename_column(\"label\", \"labels\")\n",
        "\n",
        "    # trainloader와 testloader 부분입니다.\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "    trainloader = DataLoader(\n",
        "        partition_train_test[\"train\"],\n",
        "        shuffle=True,\n",
        "        batch_size=32,\n",
        "        collate_fn=data_collator,\n",
        "    )\n",
        "\n",
        "    testloader = DataLoader(\n",
        "        partition_train_test[\"test\"], batch_size=32, collate_fn=data_collator\n",
        "    )\n",
        "\n",
        "    return trainloader, testloader\n",
        "\n",
        "# model_name에는 \"prajjwal1/bert-tiny\"가 들어갑니다.\n",
        "def get_model(model_name):\n",
        "    return AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "\n",
        "# 서버에서 초기 글로벌 모델 초기화에 사용합니다.\n",
        "def get_params(model):\n",
        "    return [val.cpu().numpy() for _, val in model.state_dict().items()]\n",
        "\n",
        "\n",
        "def set_params(model, parameters) -> None:\n",
        "    params_dict = zip(model.state_dict().keys(), parameters)\n",
        "    state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
        "    model.load_state_dict(state_dict, strict=True)\n",
        "\n",
        "\n",
        "def train(net, trainloader, epochs, device) -> None:\n",
        "    optimizer = AdamW(net.parameters(), lr=5e-5)\n",
        "    net.train()\n",
        "    for _ in range(epochs):\n",
        "        for batch in trainloader:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = net(**batch)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "\n",
        "def test(net, testloader, device) -> tuple[Any | float, Any]:\n",
        "    metric = load_metric(\"accuracy\")\n",
        "    loss = 0\n",
        "    net.eval()\n",
        "    for batch in testloader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        with torch.no_grad():\n",
        "            outputs = net(**batch)\n",
        "        logits = outputs.logits\n",
        "        loss += outputs.loss.item()\n",
        "        predictions = torch.argmax(logits, dim=-1)\n",
        "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
        "    loss /= len(testloader.dataset)\n",
        "    accuracy = metric.compute()[\"accuracy\"]\n",
        "    return loss, accuracy\n"
      ],
      "metadata": {
        "id": "HMhUy_tK8Fww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"huggingface_example: A Flower / Hugging Face app.\"\"\"\n",
        "\n",
        "import warnings\n",
        "\n",
        "import torch\n",
        "from flwr.client import Client, ClientApp, NumPyClient\n",
        "from flwr.common import Context\n",
        "from transformers import logging\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# To mute warnings reminding that we need to train the model to a downstream task\n",
        "# This is something this example does.\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "\n",
        "# Flower client\n",
        "class IMDBClient(NumPyClient):\n",
        "    def __init__(self, model_name, trainloader, testloader) -> None:\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.trainloader = trainloader\n",
        "        self.testloader = testloader\n",
        "        self.net = get_model(model_name)\n",
        "        self.net.to(self.device)\n",
        "\n",
        "    def fit(self, parameters, config) -> tuple[list, int, dict]:\n",
        "        set_params(self.net, parameters)\n",
        "        train(self.net, self.trainloader, epochs=1, device=self.device)\n",
        "        return get_params(self.net), len(self.trainloader), {}\n",
        "\n",
        "    def evaluate(self, parameters, config) -> tuple[float, int, dict[str, float]]:\n",
        "        set_params(self.net, parameters)\n",
        "        loss, accuracy = test(self.net, self.testloader, device=self.device)\n",
        "        return float(loss), len(self.testloader), {\"accuracy\": float(accuracy)}\n",
        "\n",
        "def client_fn(context: Context) -> Client:\n",
        "    \"\"\"Construct a Client that will be run in a ClientApp.\"\"\"\n",
        "    # Read the node_config to fetch data partition associated to this node\n",
        "    partition_id = context.node_config[\"partition-id\"]\n",
        "    num_partitions = context.node_config[\"num-partitions\"] # 기본값은 100입니다.\n",
        "    # Read the run config to get settings to configure the Client\n",
        "    model_name = \"prajjwal1/bert-tiny\"  #context.run_config[\"model-name\"]\n",
        "    # 위 라인에서 부분 주석 처리된 이유는 context.run_config에 해당 key가 없어서 직접 입력한 것입니다. 입력 근거는 https://github.com/adap/flower/tree/main/examples/quickstart-huggingface 에서 pyproject.toml를 참고했습니다. 다른 부분들도 다 동일합니다.\n",
        "    trainloader, testloader = load_data(partition_id, num_partitions, model_name)   # 이 코드로 총 100개의 데이터셋 파티션이 생성됩니다.\n",
        "\n",
        "    return IMDBClient(model_name, trainloader, testloader).to_client()\n",
        "\n",
        "\n",
        "client_app = ClientApp(client_fn=client_fn)\n"
      ],
      "metadata": {
        "id": "Ez5lcwyE8ZNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flwr.common import Context, ndarrays_to_parameters\n",
        "from flwr.server import ServerApp, ServerAppComponents, ServerConfig\n",
        "from flwr.server.strategy import FedAvg\n",
        "\n",
        "# from huggingface_example.task import get_params, get_model\n",
        "\n",
        "\n",
        "def server_fn(context: Context) -> ServerAppComponents:\n",
        "    \"\"\"Construct components for ServerApp.\"\"\"\n",
        "    # Construct ServerConfig\n",
        "    num_rounds = 3  #context.run_config[\"num-server-rounds\"]      총 라운드 수입니다.\n",
        "    config = ServerConfig(num_rounds=num_rounds)\n",
        "\n",
        "    # Set global model initialization\n",
        "    model_name = \"prajjwal1/bert-tiny\"  #context.run_config[\"model-name\"]\n",
        "    ndarrays = get_params(get_model(model_name))\n",
        "    global_model_init = ndarrays_to_parameters(ndarrays)\n",
        "\n",
        "    # Define strategy\n",
        "    fraction_fit = 0.05 #context.run_config[\"fraction-fit\"]\n",
        "    fraction_evaluate = 0.1 #context.run_config[\"fraction-evaluate\"]\n",
        "    strategy = FedAvg(\n",
        "        fraction_fit=fraction_fit,                       # 모델 훈련에 참여할 client 수를 정합니다. 총 파티션 수 * fraction_fit = 100 * 0.05 = 5 입니다. 다만, 제 경험상 첫 라운드는 항상 두 개의 client가 작동합니다. 아닐 수도 있습니다.\n",
        "        fraction_evaluate=fraction_evaluate,             # 모델 평가에 참여할 ~~\n",
        "        initial_parameters=global_model_init,\n",
        "    )\n",
        "\n",
        "    return ServerAppComponents(config=config, strategy=strategy)\n",
        "\n",
        "\n",
        "server_app = ServerApp(server_fn=server_fn)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzMdxR3V8jeV",
        "outputId": "a331edd8-dcd5-4e7e-d7a7-675fb18a094a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from flwr.simulation import run_simulation\n",
        "\n",
        "run_simulation(\n",
        "    server_app=server_app, client_app=client_app, num_supernodes=100\n",
        ")\n",
        "\n",
        "# 반복되는 오류: (ClientAppActor pid=5156) Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
        "#                (ClientAppActor pid=5156) You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
        "# 이는 bert-tiny 모델이 파인튜닝 없이 바로 학습에 이용되어 발생하는 경고인 것 같습니다.\n",
        "# 그 외의 경고들은 무시하셔도 될 것 같습니다. 출력창 맨 아래에 보면 FL summary가 있습니다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3DpTIz98skM",
        "outputId": "a0356776-514f-4988-b8e5-b19da1d3463d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92mINFO \u001b[0m:      Starting Flower ServerApp, config: num_rounds=3, no round_timeout\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [INIT]\n",
            "\u001b[92mINFO \u001b[0m:      Using initial global parameters provided by strategy\n",
            "\u001b[92mINFO \u001b[0m:      Starting evaluation of initial global parameters\n",
            "\u001b[92mINFO \u001b[0m:      Evaluation returned no results (`None`)\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 1]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 2 clients (out of 100)\n",
            "\u001b[36m(pid=5156)\u001b[0m 2024-12-07 02:41:11.832751: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\u001b[36m(pid=5156)\u001b[0m 2024-12-07 02:41:11.857062: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\u001b[36m(pid=5156)\u001b[0m 2024-12-07 02:41:11.864471: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[36m(pid=5156)\u001b[0m 2024-12-07 02:41:13.295017: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m /usr/local/lib/python3.10/dist-packages/jupyter_client/connect.py:28: DeprecationWarning: Jupyter is migrating its paths to use standard platformdirs\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m given by the platformdirs library.  To remove this warning and\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m see the appropriate new directories, set the environment variable\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m `JUPYTER_PLATFORM_DIRS=1` and then run `jupyter --paths`.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m The use of platformdirs will be the default in `jupyter_core` v6\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m   from jupyter_core.paths import jupyter_data_dir, jupyter_runtime_dir, secure_write\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m /usr/local/lib/python3.10/dist-packages/flwr_datasets/utils.py:109: UserWarning: The currently tested dataset are ['mnist', 'ylecun/mnist', 'cifar10', 'uoft-cs/cifar10', 'fashion_mnist', 'zalando-datasets/fashion_mnist', 'sasha/dog-food', 'zh-plus/tiny-imagenet', 'scikit-learn/adult-census-income', 'cifar100', 'uoft-cs/cifar100', 'svhn', 'ufldl-stanford/svhn', 'sentiment140', 'stanfordnlp/sentiment140', 'speech_commands', 'LIUM/tedlium', 'flwrlabs/femnist', 'flwrlabs/ucf101', 'flwrlabs/ambient-acoustic-context', 'jlh/uci-mushrooms', 'Mike0307/MNIST-M', 'flwrlabs/usps', 'scikit-learn/iris', 'flwrlabs/pacs', 'flwrlabs/cinic10', 'flwrlabs/caltech101', 'flwrlabs/office-home', 'flwrlabs/fed-isic2019']. Given: stanfordnlp/imdb.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m   warnings.warn(\n",
            "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]\n",
            "Generating train split:  44%|████▍     | 11000/25000 [00:00<00:00, 103139.16 examples/s]\n",
            "Generating train split: 100%|██████████| 25000/25000 [00:00<00:00, 112420.58 examples/s]\n",
            "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]\n",
            "Generating test split:  52%|█████▏    | 13000/25000 [00:00<00:00, 120323.04 examples/s]\n",
            "Generating test split: 100%|██████████| 25000/25000 [00:00<00:00, 114848.75 examples/s]\n",
            "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]\n",
            "Generating unsupervised split:  26%|██▌       | 13000/50000 [00:00<00:00, 117152.28 examples/s]\n",
            "Generating unsupervised split:  52%|█████▏    | 26000/50000 [00:00<00:00, 116286.77 examples/s]\n",
            "Generating unsupervised split: 100%|██████████| 50000/50000 [00:00<00:00, 121013.51 examples/s]\n",
            "Map:   0%|          | 0/200 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 200/200 [00:00<00:00, 932.00 examples/s]\n",
            "Map: 100%|██████████| 50/50 [00:00<00:00, 950.84 examples/s]\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m /usr/local/lib/python3.10/dist-packages/flwr_datasets/utils.py:109: UserWarning: The currently tested dataset are ['mnist', 'ylecun/mnist', 'cifar10', 'uoft-cs/cifar10', 'fashion_mnist', 'zalando-datasets/fashion_mnist', 'sasha/dog-food', 'zh-plus/tiny-imagenet', 'scikit-learn/adult-census-income', 'cifar100', 'uoft-cs/cifar100', 'svhn', 'ufldl-stanford/svhn', 'sentiment140', 'stanfordnlp/sentiment140', 'speech_commands', 'LIUM/tedlium', 'flwrlabs/femnist', 'flwrlabs/ucf101', 'flwrlabs/ambient-acoustic-context', 'jlh/uci-mushrooms', 'Mike0307/MNIST-M', 'flwrlabs/usps', 'scikit-learn/iris', 'flwrlabs/pacs', 'flwrlabs/cinic10', 'flwrlabs/caltech101', 'flwrlabs/office-home', 'flwrlabs/fed-isic2019']. Given: stanfordnlp/imdb.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m   warnings.warn(\n",
            "Map:   0%|          | 0/200 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 200/200 [00:00<00:00, 542.01 examples/s]\n",
            "Map:   0%|          | 0/50 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 50/50 [00:00<00:00, 576.98 examples/s]\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 2 results and 0 failures\n",
            "\u001b[93mWARNING \u001b[0m:   No fit_metrics_aggregation_fn provided\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 10 clients (out of 100)\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m /usr/local/lib/python3.10/dist-packages/flwr_datasets/utils.py:109: UserWarning: The currently tested dataset are ['mnist', 'ylecun/mnist', 'cifar10', 'uoft-cs/cifar10', 'fashion_mnist', 'zalando-datasets/fashion_mnist', 'sasha/dog-food', 'zh-plus/tiny-imagenet', 'scikit-learn/adult-census-income', 'cifar100', 'uoft-cs/cifar100', 'svhn', 'ufldl-stanford/svhn', 'sentiment140', 'stanfordnlp/sentiment140', 'speech_commands', 'LIUM/tedlium', 'flwrlabs/femnist', 'flwrlabs/ucf101', 'flwrlabs/ambient-acoustic-context', 'jlh/uci-mushrooms', 'Mike0307/MNIST-M', 'flwrlabs/usps', 'scikit-learn/iris', 'flwrlabs/pacs', 'flwrlabs/cinic10', 'flwrlabs/caltech101', 'flwrlabs/office-home', 'flwrlabs/fed-isic2019']. Given: stanfordnlp/imdb.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m   warnings.warn(\n",
            "Map:   0%|          | 0/200 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 200/200 [00:00<00:00, 1053.31 examples/s]\n",
            "Map:   0%|          | 0/50 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 50/50 [00:00<00:00, 848.93 examples/s]\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Downloading builder script: 100%|██████████| 4.20k/4.20k [00:00<00:00, 15.7MB/s]\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m /usr/local/lib/python3.10/dist-packages/flwr_datasets/utils.py:109: UserWarning: The currently tested dataset are ['mnist', 'ylecun/mnist', 'cifar10', 'uoft-cs/cifar10', 'fashion_mnist', 'zalando-datasets/fashion_mnist', 'sasha/dog-food', 'zh-plus/tiny-imagenet', 'scikit-learn/adult-census-income', 'cifar100', 'uoft-cs/cifar100', 'svhn', 'ufldl-stanford/svhn', 'sentiment140', 'stanfordnlp/sentiment140', 'speech_commands', 'LIUM/tedlium', 'flwrlabs/femnist', 'flwrlabs/ucf101', 'flwrlabs/ambient-acoustic-context', 'jlh/uci-mushrooms', 'Mike0307/MNIST-M', 'flwrlabs/usps', 'scikit-learn/iris', 'flwrlabs/pacs', 'flwrlabs/cinic10', 'flwrlabs/caltech101', 'flwrlabs/office-home', 'flwrlabs/fed-isic2019']. Given: stanfordnlp/imdb.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m   warnings.warn(\n",
            "Map:   0%|          | 0/200 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 200/200 [00:00<00:00, 1016.57 examples/s]\n",
            "Map:   0%|          | 0/50 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 50/50 [00:00<00:00, 767.54 examples/s]\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m /usr/local/lib/python3.10/dist-packages/flwr_datasets/utils.py:109: UserWarning: The currently tested dataset are ['mnist', 'ylecun/mnist', 'cifar10', 'uoft-cs/cifar10', 'fashion_mnist', 'zalando-datasets/fashion_mnist', 'sasha/dog-food', 'zh-plus/tiny-imagenet', 'scikit-learn/adult-census-income', 'cifar100', 'uoft-cs/cifar100', 'svhn', 'ufldl-stanford/svhn', 'sentiment140', 'stanfordnlp/sentiment140', 'speech_commands', 'LIUM/tedlium', 'flwrlabs/femnist', 'flwrlabs/ucf101', 'flwrlabs/ambient-acoustic-context', 'jlh/uci-mushrooms', 'Mike0307/MNIST-M', 'flwrlabs/usps', 'scikit-learn/iris', 'flwrlabs/pacs', 'flwrlabs/cinic10', 'flwrlabs/caltech101', 'flwrlabs/office-home', 'flwrlabs/fed-isic2019']. Given: stanfordnlp/imdb.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m   warnings.warn(\n",
            "Map:   0%|          | 0/200 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 200/200 [00:00<00:00, 940.25 examples/s]\n",
            "Map: 100%|██████████| 50/50 [00:00<00:00, 772.25 examples/s]\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m /usr/local/lib/python3.10/dist-packages/flwr_datasets/utils.py:109: UserWarning: The currently tested dataset are ['mnist', 'ylecun/mnist', 'cifar10', 'uoft-cs/cifar10', 'fashion_mnist', 'zalando-datasets/fashion_mnist', 'sasha/dog-food', 'zh-plus/tiny-imagenet', 'scikit-learn/adult-census-income', 'cifar100', 'uoft-cs/cifar100', 'svhn', 'ufldl-stanford/svhn', 'sentiment140', 'stanfordnlp/sentiment140', 'speech_commands', 'LIUM/tedlium', 'flwrlabs/femnist', 'flwrlabs/ucf101', 'flwrlabs/ambient-acoustic-context', 'jlh/uci-mushrooms', 'Mike0307/MNIST-M', 'flwrlabs/usps', 'scikit-learn/iris', 'flwrlabs/pacs', 'flwrlabs/cinic10', 'flwrlabs/caltech101', 'flwrlabs/office-home', 'flwrlabs/fed-isic2019']. Given: stanfordnlp/imdb.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m   warnings.warn(\n",
            "Map:   0%|          | 0/200 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 200/200 [00:00<00:00, 537.31 examples/s]\n",
            "Map:   0%|          | 0/50 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 50/50 [00:00<00:00, 456.21 examples/s]\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m /usr/local/lib/python3.10/dist-packages/flwr_datasets/utils.py:109: UserWarning: The currently tested dataset are ['mnist', 'ylecun/mnist', 'cifar10', 'uoft-cs/cifar10', 'fashion_mnist', 'zalando-datasets/fashion_mnist', 'sasha/dog-food', 'zh-plus/tiny-imagenet', 'scikit-learn/adult-census-income', 'cifar100', 'uoft-cs/cifar100', 'svhn', 'ufldl-stanford/svhn', 'sentiment140', 'stanfordnlp/sentiment140', 'speech_commands', 'LIUM/tedlium', 'flwrlabs/femnist', 'flwrlabs/ucf101', 'flwrlabs/ambient-acoustic-context', 'jlh/uci-mushrooms', 'Mike0307/MNIST-M', 'flwrlabs/usps', 'scikit-learn/iris', 'flwrlabs/pacs', 'flwrlabs/cinic10', 'flwrlabs/caltech101', 'flwrlabs/office-home', 'flwrlabs/fed-isic2019']. Given: stanfordnlp/imdb.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m   warnings.warn(\n",
            "Map:   0%|          | 0/200 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 200/200 [00:00<00:00, 918.98 examples/s]\n",
            "Map:   0%|          | 0/50 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 50/50 [00:00<00:00, 759.77 examples/s]\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m /usr/local/lib/python3.10/dist-packages/flwr_datasets/utils.py:109: UserWarning: The currently tested dataset are ['mnist', 'ylecun/mnist', 'cifar10', 'uoft-cs/cifar10', 'fashion_mnist', 'zalando-datasets/fashion_mnist', 'sasha/dog-food', 'zh-plus/tiny-imagenet', 'scikit-learn/adult-census-income', 'cifar100', 'uoft-cs/cifar100', 'svhn', 'ufldl-stanford/svhn', 'sentiment140', 'stanfordnlp/sentiment140', 'speech_commands', 'LIUM/tedlium', 'flwrlabs/femnist', 'flwrlabs/ucf101', 'flwrlabs/ambient-acoustic-context', 'jlh/uci-mushrooms', 'Mike0307/MNIST-M', 'flwrlabs/usps', 'scikit-learn/iris', 'flwrlabs/pacs', 'flwrlabs/cinic10', 'flwrlabs/caltech101', 'flwrlabs/office-home', 'flwrlabs/fed-isic2019']. Given: stanfordnlp/imdb.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m   warnings.warn(\n",
            "Map:   0%|          | 0/200 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 200/200 [00:00<00:00, 1019.68 examples/s]\n",
            "Map:   0%|          | 0/50 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 50/50 [00:00<00:00, 744.44 examples/s]\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m /usr/local/lib/python3.10/dist-packages/flwr_datasets/utils.py:109: UserWarning: The currently tested dataset are ['mnist', 'ylecun/mnist', 'cifar10', 'uoft-cs/cifar10', 'fashion_mnist', 'zalando-datasets/fashion_mnist', 'sasha/dog-food', 'zh-plus/tiny-imagenet', 'scikit-learn/adult-census-income', 'cifar100', 'uoft-cs/cifar100', 'svhn', 'ufldl-stanford/svhn', 'sentiment140', 'stanfordnlp/sentiment140', 'speech_commands', 'LIUM/tedlium', 'flwrlabs/femnist', 'flwrlabs/ucf101', 'flwrlabs/ambient-acoustic-context', 'jlh/uci-mushrooms', 'Mike0307/MNIST-M', 'flwrlabs/usps', 'scikit-learn/iris', 'flwrlabs/pacs', 'flwrlabs/cinic10', 'flwrlabs/caltech101', 'flwrlabs/office-home', 'flwrlabs/fed-isic2019']. Given: stanfordnlp/imdb.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m   warnings.warn(\n",
            "Map:   0%|          | 0/200 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 200/200 [00:00<00:00, 1000.00 examples/s]\n",
            "Map: 100%|██████████| 50/50 [00:00<00:00, 967.62 examples/s]\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m /usr/local/lib/python3.10/dist-packages/flwr_datasets/utils.py:109: UserWarning: The currently tested dataset are ['mnist', 'ylecun/mnist', 'cifar10', 'uoft-cs/cifar10', 'fashion_mnist', 'zalando-datasets/fashion_mnist', 'sasha/dog-food', 'zh-plus/tiny-imagenet', 'scikit-learn/adult-census-income', 'cifar100', 'uoft-cs/cifar100', 'svhn', 'ufldl-stanford/svhn', 'sentiment140', 'stanfordnlp/sentiment140', 'speech_commands', 'LIUM/tedlium', 'flwrlabs/femnist', 'flwrlabs/ucf101', 'flwrlabs/ambient-acoustic-context', 'jlh/uci-mushrooms', 'Mike0307/MNIST-M', 'flwrlabs/usps', 'scikit-learn/iris', 'flwrlabs/pacs', 'flwrlabs/cinic10', 'flwrlabs/caltech101', 'flwrlabs/office-home', 'flwrlabs/fed-isic2019']. Given: stanfordnlp/imdb.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m   warnings.warn(\n",
            "Map:   0%|          | 0/200 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 200/200 [00:00<00:00, 1077.90 examples/s]\n",
            "Map:   0%|          | 0/50 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 50/50 [00:00<00:00, 758.93 examples/s]\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m /usr/local/lib/python3.10/dist-packages/flwr_datasets/utils.py:109: UserWarning: The currently tested dataset are ['mnist', 'ylecun/mnist', 'cifar10', 'uoft-cs/cifar10', 'fashion_mnist', 'zalando-datasets/fashion_mnist', 'sasha/dog-food', 'zh-plus/tiny-imagenet', 'scikit-learn/adult-census-income', 'cifar100', 'uoft-cs/cifar100', 'svhn', 'ufldl-stanford/svhn', 'sentiment140', 'stanfordnlp/sentiment140', 'speech_commands', 'LIUM/tedlium', 'flwrlabs/femnist', 'flwrlabs/ucf101', 'flwrlabs/ambient-acoustic-context', 'jlh/uci-mushrooms', 'Mike0307/MNIST-M', 'flwrlabs/usps', 'scikit-learn/iris', 'flwrlabs/pacs', 'flwrlabs/cinic10', 'flwrlabs/caltech101', 'flwrlabs/office-home', 'flwrlabs/fed-isic2019']. Given: stanfordnlp/imdb.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m   warnings.warn(\n",
            "Map:   0%|          | 0/200 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 200/200 [00:00<00:00, 593.11 examples/s]\n",
            "Map:   0%|          | 0/50 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 50/50 [00:00<00:00, 512.04 examples/s]\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m /usr/local/lib/python3.10/dist-packages/flwr_datasets/utils.py:109: UserWarning: The currently tested dataset are ['mnist', 'ylecun/mnist', 'cifar10', 'uoft-cs/cifar10', 'fashion_mnist', 'zalando-datasets/fashion_mnist', 'sasha/dog-food', 'zh-plus/tiny-imagenet', 'scikit-learn/adult-census-income', 'cifar100', 'uoft-cs/cifar100', 'svhn', 'ufldl-stanford/svhn', 'sentiment140', 'stanfordnlp/sentiment140', 'speech_commands', 'LIUM/tedlium', 'flwrlabs/femnist', 'flwrlabs/ucf101', 'flwrlabs/ambient-acoustic-context', 'jlh/uci-mushrooms', 'Mike0307/MNIST-M', 'flwrlabs/usps', 'scikit-learn/iris', 'flwrlabs/pacs', 'flwrlabs/cinic10', 'flwrlabs/caltech101', 'flwrlabs/office-home', 'flwrlabs/fed-isic2019']. Given: stanfordnlp/imdb.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m   warnings.warn(\n",
            "Map:   0%|          | 0/200 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 200/200 [00:00<00:00, 975.79 examples/s]\n",
            "Map:   0%|          | 0/50 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 50/50 [00:00<00:00, 829.24 examples/s]\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 10 results and 0 failures\n",
            "\u001b[93mWARNING \u001b[0m:   No evaluate_metrics_aggregation_fn provided\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 2]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 100)\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m /usr/local/lib/python3.10/dist-packages/flwr_datasets/utils.py:109: UserWarning: The currently tested dataset are ['mnist', 'ylecun/mnist', 'cifar10', 'uoft-cs/cifar10', 'fashion_mnist', 'zalando-datasets/fashion_mnist', 'sasha/dog-food', 'zh-plus/tiny-imagenet', 'scikit-learn/adult-census-income', 'cifar100', 'uoft-cs/cifar100', 'svhn', 'ufldl-stanford/svhn', 'sentiment140', 'stanfordnlp/sentiment140', 'speech_commands', 'LIUM/tedlium', 'flwrlabs/femnist', 'flwrlabs/ucf101', 'flwrlabs/ambient-acoustic-context', 'jlh/uci-mushrooms', 'Mike0307/MNIST-M', 'flwrlabs/usps', 'scikit-learn/iris', 'flwrlabs/pacs', 'flwrlabs/cinic10', 'flwrlabs/caltech101', 'flwrlabs/office-home', 'flwrlabs/fed-isic2019']. Given: stanfordnlp/imdb.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m   warnings.warn(\n",
            "Map:   0%|          | 0/200 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 200/200 [00:00<00:00, 996.58 examples/s] \n",
            "Map: 100%|██████████| 50/50 [00:00<00:00, 862.73 examples/s]\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m /usr/local/lib/python3.10/dist-packages/flwr_datasets/utils.py:109: UserWarning: The currently tested dataset are ['mnist', 'ylecun/mnist', 'cifar10', 'uoft-cs/cifar10', 'fashion_mnist', 'zalando-datasets/fashion_mnist', 'sasha/dog-food', 'zh-plus/tiny-imagenet', 'scikit-learn/adult-census-income', 'cifar100', 'uoft-cs/cifar100', 'svhn', 'ufldl-stanford/svhn', 'sentiment140', 'stanfordnlp/sentiment140', 'speech_commands', 'LIUM/tedlium', 'flwrlabs/femnist', 'flwrlabs/ucf101', 'flwrlabs/ambient-acoustic-context', 'jlh/uci-mushrooms', 'Mike0307/MNIST-M', 'flwrlabs/usps', 'scikit-learn/iris', 'flwrlabs/pacs', 'flwrlabs/cinic10', 'flwrlabs/caltech101', 'flwrlabs/office-home', 'flwrlabs/fed-isic2019']. Given: stanfordnlp/imdb.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m   warnings.warn(\n",
            "Map:   0%|          | 0/200 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 200/200 [00:00<00:00, 1019.89 examples/s]\n",
            "Map:   0%|          | 0/50 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 50/50 [00:00<00:00, 787.20 examples/s]\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m /usr/local/lib/python3.10/dist-packages/flwr_datasets/utils.py:109: UserWarning: The currently tested dataset are ['mnist', 'ylecun/mnist', 'cifar10', 'uoft-cs/cifar10', 'fashion_mnist', 'zalando-datasets/fashion_mnist', 'sasha/dog-food', 'zh-plus/tiny-imagenet', 'scikit-learn/adult-census-income', 'cifar100', 'uoft-cs/cifar100', 'svhn', 'ufldl-stanford/svhn', 'sentiment140', 'stanfordnlp/sentiment140', 'speech_commands', 'LIUM/tedlium', 'flwrlabs/femnist', 'flwrlabs/ucf101', 'flwrlabs/ambient-acoustic-context', 'jlh/uci-mushrooms', 'Mike0307/MNIST-M', 'flwrlabs/usps', 'scikit-learn/iris', 'flwrlabs/pacs', 'flwrlabs/cinic10', 'flwrlabs/caltech101', 'flwrlabs/office-home', 'flwrlabs/fed-isic2019']. Given: stanfordnlp/imdb.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m   warnings.warn(\n",
            "Map:   0%|          | 0/200 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 200/200 [00:00<00:00, 502.16 examples/s]\n",
            "Map: 100%|██████████| 200/200 [00:00<00:00, 493.69 examples/s]\n",
            "Map:   0%|          | 0/50 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 50/50 [00:00<00:00, 475.47 examples/s]\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m /usr/local/lib/python3.10/dist-packages/flwr_datasets/utils.py:109: UserWarning: The currently tested dataset are ['mnist', 'ylecun/mnist', 'cifar10', 'uoft-cs/cifar10', 'fashion_mnist', 'zalando-datasets/fashion_mnist', 'sasha/dog-food', 'zh-plus/tiny-imagenet', 'scikit-learn/adult-census-income', 'cifar100', 'uoft-cs/cifar100', 'svhn', 'ufldl-stanford/svhn', 'sentiment140', 'stanfordnlp/sentiment140', 'speech_commands', 'LIUM/tedlium', 'flwrlabs/femnist', 'flwrlabs/ucf101', 'flwrlabs/ambient-acoustic-context', 'jlh/uci-mushrooms', 'Mike0307/MNIST-M', 'flwrlabs/usps', 'scikit-learn/iris', 'flwrlabs/pacs', 'flwrlabs/cinic10', 'flwrlabs/caltech101', 'flwrlabs/office-home', 'flwrlabs/fed-isic2019']. Given: stanfordnlp/imdb.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m   warnings.warn(\n",
            "Map:   0%|          | 0/200 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 200/200 [00:00<00:00, 1030.57 examples/s]\n",
            "Map:   0%|          | 0/50 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 50/50 [00:00<00:00, 787.55 examples/s]\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m /usr/local/lib/python3.10/dist-packages/flwr_datasets/utils.py:109: UserWarning: The currently tested dataset are ['mnist', 'ylecun/mnist', 'cifar10', 'uoft-cs/cifar10', 'fashion_mnist', 'zalando-datasets/fashion_mnist', 'sasha/dog-food', 'zh-plus/tiny-imagenet', 'scikit-learn/adult-census-income', 'cifar100', 'uoft-cs/cifar100', 'svhn', 'ufldl-stanford/svhn', 'sentiment140', 'stanfordnlp/sentiment140', 'speech_commands', 'LIUM/tedlium', 'flwrlabs/femnist', 'flwrlabs/ucf101', 'flwrlabs/ambient-acoustic-context', 'jlh/uci-mushrooms', 'Mike0307/MNIST-M', 'flwrlabs/usps', 'scikit-learn/iris', 'flwrlabs/pacs', 'flwrlabs/cinic10', 'flwrlabs/caltech101', 'flwrlabs/office-home', 'flwrlabs/fed-isic2019']. Given: stanfordnlp/imdb.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m   warnings.warn(\n",
            "Map:   0%|          | 0/50 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 50/50 [00:00<00:00, 863.48 examples/s]\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 10 clients (out of 100)\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m /usr/local/lib/python3.10/dist-packages/flwr_datasets/utils.py:109: UserWarning: The currently tested dataset are ['mnist', 'ylecun/mnist', 'cifar10', 'uoft-cs/cifar10', 'fashion_mnist', 'zalando-datasets/fashion_mnist', 'sasha/dog-food', 'zh-plus/tiny-imagenet', 'scikit-learn/adult-census-income', 'cifar100', 'uoft-cs/cifar100', 'svhn', 'ufldl-stanford/svhn', 'sentiment140', 'stanfordnlp/sentiment140', 'speech_commands', 'LIUM/tedlium', 'flwrlabs/femnist', 'flwrlabs/ucf101', 'flwrlabs/ambient-acoustic-context', 'jlh/uci-mushrooms', 'Mike0307/MNIST-M', 'flwrlabs/usps', 'scikit-learn/iris', 'flwrlabs/pacs', 'flwrlabs/cinic10', 'flwrlabs/caltech101', 'flwrlabs/office-home', 'flwrlabs/fed-isic2019']. Given: stanfordnlp/imdb.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m   warnings.warn(\n",
            "Map:   0%|          | 0/200 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 200/200 [00:00<00:00, 872.91 examples/s]\n",
            "Map: 100%|██████████| 50/50 [00:00<00:00, 843.98 examples/s]\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m /usr/local/lib/python3.10/dist-packages/flwr_datasets/utils.py:109: UserWarning: The currently tested dataset are ['mnist', 'ylecun/mnist', 'cifar10', 'uoft-cs/cifar10', 'fashion_mnist', 'zalando-datasets/fashion_mnist', 'sasha/dog-food', 'zh-plus/tiny-imagenet', 'scikit-learn/adult-census-income', 'cifar100', 'uoft-cs/cifar100', 'svhn', 'ufldl-stanford/svhn', 'sentiment140', 'stanfordnlp/sentiment140', 'speech_commands', 'LIUM/tedlium', 'flwrlabs/femnist', 'flwrlabs/ucf101', 'flwrlabs/ambient-acoustic-context', 'jlh/uci-mushrooms', 'Mike0307/MNIST-M', 'flwrlabs/usps', 'scikit-learn/iris', 'flwrlabs/pacs', 'flwrlabs/cinic10', 'flwrlabs/caltech101', 'flwrlabs/office-home', 'flwrlabs/fed-isic2019']. Given: stanfordnlp/imdb.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m   warnings.warn(\n",
            "Map:   0%|          | 0/200 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 200/200 [00:00<00:00, 1045.62 examples/s]\n",
            "Map:   0%|          | 0/50 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 50/50 [00:00<00:00, 674.51 examples/s]\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m /usr/local/lib/python3.10/dist-packages/flwr_datasets/utils.py:109: UserWarning: The currently tested dataset are ['mnist', 'ylecun/mnist', 'cifar10', 'uoft-cs/cifar10', 'fashion_mnist', 'zalando-datasets/fashion_mnist', 'sasha/dog-food', 'zh-plus/tiny-imagenet', 'scikit-learn/adult-census-income', 'cifar100', 'uoft-cs/cifar100', 'svhn', 'ufldl-stanford/svhn', 'sentiment140', 'stanfordnlp/sentiment140', 'speech_commands', 'LIUM/tedlium', 'flwrlabs/femnist', 'flwrlabs/ucf101', 'flwrlabs/ambient-acoustic-context', 'jlh/uci-mushrooms', 'Mike0307/MNIST-M', 'flwrlabs/usps', 'scikit-learn/iris', 'flwrlabs/pacs', 'flwrlabs/cinic10', 'flwrlabs/caltech101', 'flwrlabs/office-home', 'flwrlabs/fed-isic2019']. Given: stanfordnlp/imdb.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m   warnings.warn(\n",
            "Map:   0%|          | 0/200 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 200/200 [00:00<00:00, 926.32 examples/s]\n",
            "Map:   0%|          | 0/50 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 50/50 [00:00<00:00, 914.68 examples/s]\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m /usr/local/lib/python3.10/dist-packages/flwr_datasets/utils.py:109: UserWarning: The currently tested dataset are ['mnist', 'ylecun/mnist', 'cifar10', 'uoft-cs/cifar10', 'fashion_mnist', 'zalando-datasets/fashion_mnist', 'sasha/dog-food', 'zh-plus/tiny-imagenet', 'scikit-learn/adult-census-income', 'cifar100', 'uoft-cs/cifar100', 'svhn', 'ufldl-stanford/svhn', 'sentiment140', 'stanfordnlp/sentiment140', 'speech_commands', 'LIUM/tedlium', 'flwrlabs/femnist', 'flwrlabs/ucf101', 'flwrlabs/ambient-acoustic-context', 'jlh/uci-mushrooms', 'Mike0307/MNIST-M', 'flwrlabs/usps', 'scikit-learn/iris', 'flwrlabs/pacs', 'flwrlabs/cinic10', 'flwrlabs/caltech101', 'flwrlabs/office-home', 'flwrlabs/fed-isic2019']. Given: stanfordnlp/imdb.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m   warnings.warn(\n",
            "Map:   0%|          | 0/200 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 200/200 [00:00<00:00, 922.96 examples/s]\n",
            "Map:   0%|          | 0/50 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 50/50 [00:00<00:00, 762.84 examples/s]\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m /usr/local/lib/python3.10/dist-packages/flwr_datasets/utils.py:109: UserWarning: The currently tested dataset are ['mnist', 'ylecun/mnist', 'cifar10', 'uoft-cs/cifar10', 'fashion_mnist', 'zalando-datasets/fashion_mnist', 'sasha/dog-food', 'zh-plus/tiny-imagenet', 'scikit-learn/adult-census-income', 'cifar100', 'uoft-cs/cifar100', 'svhn', 'ufldl-stanford/svhn', 'sentiment140', 'stanfordnlp/sentiment140', 'speech_commands', 'LIUM/tedlium', 'flwrlabs/femnist', 'flwrlabs/ucf101', 'flwrlabs/ambient-acoustic-context', 'jlh/uci-mushrooms', 'Mike0307/MNIST-M', 'flwrlabs/usps', 'scikit-learn/iris', 'flwrlabs/pacs', 'flwrlabs/cinic10', 'flwrlabs/caltech101', 'flwrlabs/office-home', 'flwrlabs/fed-isic2019']. Given: stanfordnlp/imdb.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m   warnings.warn(\n",
            "Map:   0%|          | 0/200 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 200/200 [00:00<00:00, 994.71 examples/s] \n",
            "Map:   0%|          | 0/50 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 50/50 [00:00<00:00, 830.40 examples/s]\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m /usr/local/lib/python3.10/dist-packages/flwr_datasets/utils.py:109: UserWarning: The currently tested dataset are ['mnist', 'ylecun/mnist', 'cifar10', 'uoft-cs/cifar10', 'fashion_mnist', 'zalando-datasets/fashion_mnist', 'sasha/dog-food', 'zh-plus/tiny-imagenet', 'scikit-learn/adult-census-income', 'cifar100', 'uoft-cs/cifar100', 'svhn', 'ufldl-stanford/svhn', 'sentiment140', 'stanfordnlp/sentiment140', 'speech_commands', 'LIUM/tedlium', 'flwrlabs/femnist', 'flwrlabs/ucf101', 'flwrlabs/ambient-acoustic-context', 'jlh/uci-mushrooms', 'Mike0307/MNIST-M', 'flwrlabs/usps', 'scikit-learn/iris', 'flwrlabs/pacs', 'flwrlabs/cinic10', 'flwrlabs/caltech101', 'flwrlabs/office-home', 'flwrlabs/fed-isic2019']. Given: stanfordnlp/imdb.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m   warnings.warn(\n",
            "Map:   0%|          | 0/50 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 50/50 [00:00<00:00, 786.20 examples/s]\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m /usr/local/lib/python3.10/dist-packages/flwr_datasets/utils.py:109: UserWarning: The currently tested dataset are ['mnist', 'ylecun/mnist', 'cifar10', 'uoft-cs/cifar10', 'fashion_mnist', 'zalando-datasets/fashion_mnist', 'sasha/dog-food', 'zh-plus/tiny-imagenet', 'scikit-learn/adult-census-income', 'cifar100', 'uoft-cs/cifar100', 'svhn', 'ufldl-stanford/svhn', 'sentiment140', 'stanfordnlp/sentiment140', 'speech_commands', 'LIUM/tedlium', 'flwrlabs/femnist', 'flwrlabs/ucf101', 'flwrlabs/ambient-acoustic-context', 'jlh/uci-mushrooms', 'Mike0307/MNIST-M', 'flwrlabs/usps', 'scikit-learn/iris', 'flwrlabs/pacs', 'flwrlabs/cinic10', 'flwrlabs/caltech101', 'flwrlabs/office-home', 'flwrlabs/fed-isic2019']. Given: stanfordnlp/imdb.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m   warnings.warn(\n",
            "Map:   0%|          | 0/200 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 200/200 [00:00<00:00, 1032.59 examples/s]\n",
            "Map: 100%|██████████| 50/50 [00:00<00:00, 924.72 examples/s]\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m /usr/local/lib/python3.10/dist-packages/flwr_datasets/utils.py:109: UserWarning: The currently tested dataset are ['mnist', 'ylecun/mnist', 'cifar10', 'uoft-cs/cifar10', 'fashion_mnist', 'zalando-datasets/fashion_mnist', 'sasha/dog-food', 'zh-plus/tiny-imagenet', 'scikit-learn/adult-census-income', 'cifar100', 'uoft-cs/cifar100', 'svhn', 'ufldl-stanford/svhn', 'sentiment140', 'stanfordnlp/sentiment140', 'speech_commands', 'LIUM/tedlium', 'flwrlabs/femnist', 'flwrlabs/ucf101', 'flwrlabs/ambient-acoustic-context', 'jlh/uci-mushrooms', 'Mike0307/MNIST-M', 'flwrlabs/usps', 'scikit-learn/iris', 'flwrlabs/pacs', 'flwrlabs/cinic10', 'flwrlabs/caltech101', 'flwrlabs/office-home', 'flwrlabs/fed-isic2019']. Given: stanfordnlp/imdb.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m   warnings.warn(\n",
            "Map:   0%|          | 0/200 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 200/200 [00:00<00:00, 473.61 examples/s]\n",
            "Map:   0%|          | 0/50 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 50/50 [00:00<00:00, 510.11 examples/s]\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m /usr/local/lib/python3.10/dist-packages/flwr_datasets/utils.py:109: UserWarning: The currently tested dataset are ['mnist', 'ylecun/mnist', 'cifar10', 'uoft-cs/cifar10', 'fashion_mnist', 'zalando-datasets/fashion_mnist', 'sasha/dog-food', 'zh-plus/tiny-imagenet', 'scikit-learn/adult-census-income', 'cifar100', 'uoft-cs/cifar100', 'svhn', 'ufldl-stanford/svhn', 'sentiment140', 'stanfordnlp/sentiment140', 'speech_commands', 'LIUM/tedlium', 'flwrlabs/femnist', 'flwrlabs/ucf101', 'flwrlabs/ambient-acoustic-context', 'jlh/uci-mushrooms', 'Mike0307/MNIST-M', 'flwrlabs/usps', 'scikit-learn/iris', 'flwrlabs/pacs', 'flwrlabs/cinic10', 'flwrlabs/caltech101', 'flwrlabs/office-home', 'flwrlabs/fed-isic2019']. Given: stanfordnlp/imdb.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m   warnings.warn(\n",
            "Map:   0%|          | 0/200 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 200/200 [00:00<00:00, 957.06 examples/s]\n",
            "Map:   0%|          | 0/50 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 50/50 [00:00<00:00, 816.73 examples/s]\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m /usr/local/lib/python3.10/dist-packages/flwr_datasets/utils.py:109: UserWarning: The currently tested dataset are ['mnist', 'ylecun/mnist', 'cifar10', 'uoft-cs/cifar10', 'fashion_mnist', 'zalando-datasets/fashion_mnist', 'sasha/dog-food', 'zh-plus/tiny-imagenet', 'scikit-learn/adult-census-income', 'cifar100', 'uoft-cs/cifar100', 'svhn', 'ufldl-stanford/svhn', 'sentiment140', 'stanfordnlp/sentiment140', 'speech_commands', 'LIUM/tedlium', 'flwrlabs/femnist', 'flwrlabs/ucf101', 'flwrlabs/ambient-acoustic-context', 'jlh/uci-mushrooms', 'Mike0307/MNIST-M', 'flwrlabs/usps', 'scikit-learn/iris', 'flwrlabs/pacs', 'flwrlabs/cinic10', 'flwrlabs/caltech101', 'flwrlabs/office-home', 'flwrlabs/fed-isic2019']. Given: stanfordnlp/imdb.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m   warnings.warn(\n",
            "Map:   0%|          | 0/200 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 200/200 [00:00<00:00, 967.98 examples/s]\n",
            "Map: 100%|██████████| 50/50 [00:00<00:00, 809.26 examples/s]\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 10 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 3]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 100)\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m /usr/local/lib/python3.10/dist-packages/flwr_datasets/utils.py:109: UserWarning: The currently tested dataset are ['mnist', 'ylecun/mnist', 'cifar10', 'uoft-cs/cifar10', 'fashion_mnist', 'zalando-datasets/fashion_mnist', 'sasha/dog-food', 'zh-plus/tiny-imagenet', 'scikit-learn/adult-census-income', 'cifar100', 'uoft-cs/cifar100', 'svhn', 'ufldl-stanford/svhn', 'sentiment140', 'stanfordnlp/sentiment140', 'speech_commands', 'LIUM/tedlium', 'flwrlabs/femnist', 'flwrlabs/ucf101', 'flwrlabs/ambient-acoustic-context', 'jlh/uci-mushrooms', 'Mike0307/MNIST-M', 'flwrlabs/usps', 'scikit-learn/iris', 'flwrlabs/pacs', 'flwrlabs/cinic10', 'flwrlabs/caltech101', 'flwrlabs/office-home', 'flwrlabs/fed-isic2019']. Given: stanfordnlp/imdb.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m   warnings.warn(\n",
            "Map:   0%|          | 0/50 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 50/50 [00:00<00:00, 803.46 examples/s]\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m /usr/local/lib/python3.10/dist-packages/flwr_datasets/utils.py:109: UserWarning: The currently tested dataset are ['mnist', 'ylecun/mnist', 'cifar10', 'uoft-cs/cifar10', 'fashion_mnist', 'zalando-datasets/fashion_mnist', 'sasha/dog-food', 'zh-plus/tiny-imagenet', 'scikit-learn/adult-census-income', 'cifar100', 'uoft-cs/cifar100', 'svhn', 'ufldl-stanford/svhn', 'sentiment140', 'stanfordnlp/sentiment140', 'speech_commands', 'LIUM/tedlium', 'flwrlabs/femnist', 'flwrlabs/ucf101', 'flwrlabs/ambient-acoustic-context', 'jlh/uci-mushrooms', 'Mike0307/MNIST-M', 'flwrlabs/usps', 'scikit-learn/iris', 'flwrlabs/pacs', 'flwrlabs/cinic10', 'flwrlabs/caltech101', 'flwrlabs/office-home', 'flwrlabs/fed-isic2019']. Given: stanfordnlp/imdb.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m   warnings.warn(\n",
            "Map:   0%|          | 0/200 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 200/200 [00:00<00:00, 846.39 examples/s]\n",
            "Map:   0%|          | 0/50 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 50/50 [00:00<00:00, 530.96 examples/s]\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m /usr/local/lib/python3.10/dist-packages/flwr_datasets/utils.py:109: UserWarning: The currently tested dataset are ['mnist', 'ylecun/mnist', 'cifar10', 'uoft-cs/cifar10', 'fashion_mnist', 'zalando-datasets/fashion_mnist', 'sasha/dog-food', 'zh-plus/tiny-imagenet', 'scikit-learn/adult-census-income', 'cifar100', 'uoft-cs/cifar100', 'svhn', 'ufldl-stanford/svhn', 'sentiment140', 'stanfordnlp/sentiment140', 'speech_commands', 'LIUM/tedlium', 'flwrlabs/femnist', 'flwrlabs/ucf101', 'flwrlabs/ambient-acoustic-context', 'jlh/uci-mushrooms', 'Mike0307/MNIST-M', 'flwrlabs/usps', 'scikit-learn/iris', 'flwrlabs/pacs', 'flwrlabs/cinic10', 'flwrlabs/caltech101', 'flwrlabs/office-home', 'flwrlabs/fed-isic2019']. Given: stanfordnlp/imdb.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m   warnings.warn(\n",
            "Map:   0%|          | 0/200 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 200/200 [00:00<00:00, 986.61 examples/s] \n",
            "Map:   0%|          | 0/50 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 50/50 [00:00<00:00, 746.94 examples/s]\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m /usr/local/lib/python3.10/dist-packages/flwr_datasets/utils.py:109: UserWarning: The currently tested dataset are ['mnist', 'ylecun/mnist', 'cifar10', 'uoft-cs/cifar10', 'fashion_mnist', 'zalando-datasets/fashion_mnist', 'sasha/dog-food', 'zh-plus/tiny-imagenet', 'scikit-learn/adult-census-income', 'cifar100', 'uoft-cs/cifar100', 'svhn', 'ufldl-stanford/svhn', 'sentiment140', 'stanfordnlp/sentiment140', 'speech_commands', 'LIUM/tedlium', 'flwrlabs/femnist', 'flwrlabs/ucf101', 'flwrlabs/ambient-acoustic-context', 'jlh/uci-mushrooms', 'Mike0307/MNIST-M', 'flwrlabs/usps', 'scikit-learn/iris', 'flwrlabs/pacs', 'flwrlabs/cinic10', 'flwrlabs/caltech101', 'flwrlabs/office-home', 'flwrlabs/fed-isic2019']. Given: stanfordnlp/imdb.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m   warnings.warn(\n",
            "Map:   0%|          | 0/200 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 200/200 [00:00<00:00, 944.64 examples/s]\n",
            "Map:   0%|          | 0/50 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 50/50 [00:00<00:00, 639.92 examples/s]\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m /usr/local/lib/python3.10/dist-packages/flwr_datasets/utils.py:109: UserWarning: The currently tested dataset are ['mnist', 'ylecun/mnist', 'cifar10', 'uoft-cs/cifar10', 'fashion_mnist', 'zalando-datasets/fashion_mnist', 'sasha/dog-food', 'zh-plus/tiny-imagenet', 'scikit-learn/adult-census-income', 'cifar100', 'uoft-cs/cifar100', 'svhn', 'ufldl-stanford/svhn', 'sentiment140', 'stanfordnlp/sentiment140', 'speech_commands', 'LIUM/tedlium', 'flwrlabs/femnist', 'flwrlabs/ucf101', 'flwrlabs/ambient-acoustic-context', 'jlh/uci-mushrooms', 'Mike0307/MNIST-M', 'flwrlabs/usps', 'scikit-learn/iris', 'flwrlabs/pacs', 'flwrlabs/cinic10', 'flwrlabs/caltech101', 'flwrlabs/office-home', 'flwrlabs/fed-isic2019']. Given: stanfordnlp/imdb.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m   warnings.warn(\n",
            "Map:   0%|          | 0/200 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 200/200 [00:00<00:00, 935.52 examples/s]\n",
            "Map: 100%|██████████| 50/50 [00:00<00:00, 806.32 examples/s]\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 10 clients (out of 100)\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m /usr/local/lib/python3.10/dist-packages/flwr_datasets/utils.py:109: UserWarning: The currently tested dataset are ['mnist', 'ylecun/mnist', 'cifar10', 'uoft-cs/cifar10', 'fashion_mnist', 'zalando-datasets/fashion_mnist', 'sasha/dog-food', 'zh-plus/tiny-imagenet', 'scikit-learn/adult-census-income', 'cifar100', 'uoft-cs/cifar100', 'svhn', 'ufldl-stanford/svhn', 'sentiment140', 'stanfordnlp/sentiment140', 'speech_commands', 'LIUM/tedlium', 'flwrlabs/femnist', 'flwrlabs/ucf101', 'flwrlabs/ambient-acoustic-context', 'jlh/uci-mushrooms', 'Mike0307/MNIST-M', 'flwrlabs/usps', 'scikit-learn/iris', 'flwrlabs/pacs', 'flwrlabs/cinic10', 'flwrlabs/caltech101', 'flwrlabs/office-home', 'flwrlabs/fed-isic2019']. Given: stanfordnlp/imdb.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m   warnings.warn(\n",
            "Map:   0%|          | 0/50 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 50/50 [00:00<00:00, 494.15 examples/s]\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m /usr/local/lib/python3.10/dist-packages/flwr_datasets/utils.py:109: UserWarning: The currently tested dataset are ['mnist', 'ylecun/mnist', 'cifar10', 'uoft-cs/cifar10', 'fashion_mnist', 'zalando-datasets/fashion_mnist', 'sasha/dog-food', 'zh-plus/tiny-imagenet', 'scikit-learn/adult-census-income', 'cifar100', 'uoft-cs/cifar100', 'svhn', 'ufldl-stanford/svhn', 'sentiment140', 'stanfordnlp/sentiment140', 'speech_commands', 'LIUM/tedlium', 'flwrlabs/femnist', 'flwrlabs/ucf101', 'flwrlabs/ambient-acoustic-context', 'jlh/uci-mushrooms', 'Mike0307/MNIST-M', 'flwrlabs/usps', 'scikit-learn/iris', 'flwrlabs/pacs', 'flwrlabs/cinic10', 'flwrlabs/caltech101', 'flwrlabs/office-home', 'flwrlabs/fed-isic2019']. Given: stanfordnlp/imdb.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m   warnings.warn(\n",
            "Map:   0%|          | 0/50 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 50/50 [00:00<00:00, 838.50 examples/s]\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m /usr/local/lib/python3.10/dist-packages/flwr_datasets/utils.py:109: UserWarning: The currently tested dataset are ['mnist', 'ylecun/mnist', 'cifar10', 'uoft-cs/cifar10', 'fashion_mnist', 'zalando-datasets/fashion_mnist', 'sasha/dog-food', 'zh-plus/tiny-imagenet', 'scikit-learn/adult-census-income', 'cifar100', 'uoft-cs/cifar100', 'svhn', 'ufldl-stanford/svhn', 'sentiment140', 'stanfordnlp/sentiment140', 'speech_commands', 'LIUM/tedlium', 'flwrlabs/femnist', 'flwrlabs/ucf101', 'flwrlabs/ambient-acoustic-context', 'jlh/uci-mushrooms', 'Mike0307/MNIST-M', 'flwrlabs/usps', 'scikit-learn/iris', 'flwrlabs/pacs', 'flwrlabs/cinic10', 'flwrlabs/caltech101', 'flwrlabs/office-home', 'flwrlabs/fed-isic2019']. Given: stanfordnlp/imdb.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m   warnings.warn(\n",
            "Map: 100%|██████████| 50/50 [00:00<00:00, 726.98 examples/s]\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m /usr/local/lib/python3.10/dist-packages/flwr_datasets/utils.py:109: UserWarning: The currently tested dataset are ['mnist', 'ylecun/mnist', 'cifar10', 'uoft-cs/cifar10', 'fashion_mnist', 'zalando-datasets/fashion_mnist', 'sasha/dog-food', 'zh-plus/tiny-imagenet', 'scikit-learn/adult-census-income', 'cifar100', 'uoft-cs/cifar100', 'svhn', 'ufldl-stanford/svhn', 'sentiment140', 'stanfordnlp/sentiment140', 'speech_commands', 'LIUM/tedlium', 'flwrlabs/femnist', 'flwrlabs/ucf101', 'flwrlabs/ambient-acoustic-context', 'jlh/uci-mushrooms', 'Mike0307/MNIST-M', 'flwrlabs/usps', 'scikit-learn/iris', 'flwrlabs/pacs', 'flwrlabs/cinic10', 'flwrlabs/caltech101', 'flwrlabs/office-home', 'flwrlabs/fed-isic2019']. Given: stanfordnlp/imdb.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m   warnings.warn(\n",
            "Map:   0%|          | 0/200 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 200/200 [00:00<00:00, 1028.79 examples/s]\n",
            "Map: 100%|██████████| 50/50 [00:00<00:00, 850.03 examples/s]\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m /usr/local/lib/python3.10/dist-packages/flwr_datasets/utils.py:109: UserWarning: The currently tested dataset are ['mnist', 'ylecun/mnist', 'cifar10', 'uoft-cs/cifar10', 'fashion_mnist', 'zalando-datasets/fashion_mnist', 'sasha/dog-food', 'zh-plus/tiny-imagenet', 'scikit-learn/adult-census-income', 'cifar100', 'uoft-cs/cifar100', 'svhn', 'ufldl-stanford/svhn', 'sentiment140', 'stanfordnlp/sentiment140', 'speech_commands', 'LIUM/tedlium', 'flwrlabs/femnist', 'flwrlabs/ucf101', 'flwrlabs/ambient-acoustic-context', 'jlh/uci-mushrooms', 'Mike0307/MNIST-M', 'flwrlabs/usps', 'scikit-learn/iris', 'flwrlabs/pacs', 'flwrlabs/cinic10', 'flwrlabs/caltech101', 'flwrlabs/office-home', 'flwrlabs/fed-isic2019']. Given: stanfordnlp/imdb.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m   warnings.warn(\n",
            "Map:   0%|          | 0/200 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 200/200 [00:00<00:00, 989.86 examples/s] \n",
            "Map:   0%|          | 0/50 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 50/50 [00:00<00:00, 756.94 examples/s]\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m /usr/local/lib/python3.10/dist-packages/flwr_datasets/utils.py:109: UserWarning: The currently tested dataset are ['mnist', 'ylecun/mnist', 'cifar10', 'uoft-cs/cifar10', 'fashion_mnist', 'zalando-datasets/fashion_mnist', 'sasha/dog-food', 'zh-plus/tiny-imagenet', 'scikit-learn/adult-census-income', 'cifar100', 'uoft-cs/cifar100', 'svhn', 'ufldl-stanford/svhn', 'sentiment140', 'stanfordnlp/sentiment140', 'speech_commands', 'LIUM/tedlium', 'flwrlabs/femnist', 'flwrlabs/ucf101', 'flwrlabs/ambient-acoustic-context', 'jlh/uci-mushrooms', 'Mike0307/MNIST-M', 'flwrlabs/usps', 'scikit-learn/iris', 'flwrlabs/pacs', 'flwrlabs/cinic10', 'flwrlabs/caltech101', 'flwrlabs/office-home', 'flwrlabs/fed-isic2019']. Given: stanfordnlp/imdb.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m   warnings.warn(\n",
            "Map:   0%|          | 0/50 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 50/50 [00:00<00:00, 465.85 examples/s]\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m /usr/local/lib/python3.10/dist-packages/flwr_datasets/utils.py:109: UserWarning: The currently tested dataset are ['mnist', 'ylecun/mnist', 'cifar10', 'uoft-cs/cifar10', 'fashion_mnist', 'zalando-datasets/fashion_mnist', 'sasha/dog-food', 'zh-plus/tiny-imagenet', 'scikit-learn/adult-census-income', 'cifar100', 'uoft-cs/cifar100', 'svhn', 'ufldl-stanford/svhn', 'sentiment140', 'stanfordnlp/sentiment140', 'speech_commands', 'LIUM/tedlium', 'flwrlabs/femnist', 'flwrlabs/ucf101', 'flwrlabs/ambient-acoustic-context', 'jlh/uci-mushrooms', 'Mike0307/MNIST-M', 'flwrlabs/usps', 'scikit-learn/iris', 'flwrlabs/pacs', 'flwrlabs/cinic10', 'flwrlabs/caltech101', 'flwrlabs/office-home', 'flwrlabs/fed-isic2019']. Given: stanfordnlp/imdb.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m   warnings.warn(\n",
            "Map:   0%|          | 0/200 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 200/200 [00:00<00:00, 960.46 examples/s]\n",
            "Map: 100%|██████████| 200/200 [00:00<00:00, 929.18 examples/s]\n",
            "Map: 100%|██████████| 50/50 [00:00<00:00, 795.54 examples/s]\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m /usr/local/lib/python3.10/dist-packages/flwr_datasets/utils.py:109: UserWarning: The currently tested dataset are ['mnist', 'ylecun/mnist', 'cifar10', 'uoft-cs/cifar10', 'fashion_mnist', 'zalando-datasets/fashion_mnist', 'sasha/dog-food', 'zh-plus/tiny-imagenet', 'scikit-learn/adult-census-income', 'cifar100', 'uoft-cs/cifar100', 'svhn', 'ufldl-stanford/svhn', 'sentiment140', 'stanfordnlp/sentiment140', 'speech_commands', 'LIUM/tedlium', 'flwrlabs/femnist', 'flwrlabs/ucf101', 'flwrlabs/ambient-acoustic-context', 'jlh/uci-mushrooms', 'Mike0307/MNIST-M', 'flwrlabs/usps', 'scikit-learn/iris', 'flwrlabs/pacs', 'flwrlabs/cinic10', 'flwrlabs/caltech101', 'flwrlabs/office-home', 'flwrlabs/fed-isic2019']. Given: stanfordnlp/imdb.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m   warnings.warn(\n",
            "Map:   0%|          | 0/50 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 50/50 [00:00<00:00, 757.40 examples/s]\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m /usr/local/lib/python3.10/dist-packages/flwr_datasets/utils.py:109: UserWarning: The currently tested dataset are ['mnist', 'ylecun/mnist', 'cifar10', 'uoft-cs/cifar10', 'fashion_mnist', 'zalando-datasets/fashion_mnist', 'sasha/dog-food', 'zh-plus/tiny-imagenet', 'scikit-learn/adult-census-income', 'cifar100', 'uoft-cs/cifar100', 'svhn', 'ufldl-stanford/svhn', 'sentiment140', 'stanfordnlp/sentiment140', 'speech_commands', 'LIUM/tedlium', 'flwrlabs/femnist', 'flwrlabs/ucf101', 'flwrlabs/ambient-acoustic-context', 'jlh/uci-mushrooms', 'Mike0307/MNIST-M', 'flwrlabs/usps', 'scikit-learn/iris', 'flwrlabs/pacs', 'flwrlabs/cinic10', 'flwrlabs/caltech101', 'flwrlabs/office-home', 'flwrlabs/fed-isic2019']. Given: stanfordnlp/imdb.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m   warnings.warn(\n",
            "Map:   0%|          | 0/200 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 200/200 [00:00<00:00, 854.71 examples/s]\n",
            "Map:   0%|          | 0/50 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 50/50 [00:00<00:00, 707.48 examples/s]\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m /usr/local/lib/python3.10/dist-packages/flwr_datasets/utils.py:109: UserWarning: The currently tested dataset are ['mnist', 'ylecun/mnist', 'cifar10', 'uoft-cs/cifar10', 'fashion_mnist', 'zalando-datasets/fashion_mnist', 'sasha/dog-food', 'zh-plus/tiny-imagenet', 'scikit-learn/adult-census-income', 'cifar100', 'uoft-cs/cifar100', 'svhn', 'ufldl-stanford/svhn', 'sentiment140', 'stanfordnlp/sentiment140', 'speech_commands', 'LIUM/tedlium', 'flwrlabs/femnist', 'flwrlabs/ucf101', 'flwrlabs/ambient-acoustic-context', 'jlh/uci-mushrooms', 'Mike0307/MNIST-M', 'flwrlabs/usps', 'scikit-learn/iris', 'flwrlabs/pacs', 'flwrlabs/cinic10', 'flwrlabs/caltech101', 'flwrlabs/office-home', 'flwrlabs/fed-isic2019']. Given: stanfordnlp/imdb.\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m   warnings.warn(\n",
            "Map:   0%|          | 0/50 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 50/50 [00:00<00:00, 731.59 examples/s]\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "\u001b[36m(ClientAppActor pid=5156)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 10 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [SUMMARY]\n",
            "\u001b[92mINFO \u001b[0m:      Run finished 3 round(s) in 359.75s\n",
            "\u001b[92mINFO \u001b[0m:      \tHistory (loss, distributed):\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 1: 0.027545382618904112\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 2: 0.027411493062973025\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 3: 0.02690470886230469\n",
            "\u001b[92mINFO \u001b[0m:      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-Ih48DuI93ld"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}